# Regression

> 회귀, 회귀 분석



## 1. 회귀 개념

### 머신러닝 분야

- 지도학습
  - **회귀분석(Regression)**
  - 분류(Classification)
- 비지도학습
  - 클러스터링(Clustering)
  - 차원 축소(Dimensionality Reduction)
- 강화학습



### 회귀 분석이란?

데이터를 가장 <u>잘 설명하는 선</u>을 찾아 입력값에 따른 미래 결과값을 예측하는 알고리즘

**예시**

```
Y ∽ β_0 + β_1 X
X = 평균 기온, Y = 아이스크림 판매량
⇒ 적절한 β\_0(y절편)과 β_1(기울기) 찾기
```



### 기울기와 절편 찾기

**아이디어**: 완벽한 예측은 불가능함

각 데이터의 실제 값과 모델이 예측하는 값의 차이를 최소한으로 하는 선을 찾자

즉, <u>Loss function</u>을 최소로 만드는 β\_0, β\_1 구하기



### Loss function

**전체 모델의 차이**
$$
\sum (y^{(i)}-(\beta_0x^{(i)} + \beta_1))^2
$$

- (+)와 (-) 값이 서로를 상쇄하지 못하도록 하기 위해 제곱값을 이용

$$
argmin_{\beta_0\beta_1}\sum (y^{(i)}-(\beta_0x^{(i)} + \beta_1))^2
$$

- Loss function을 최소로 하는 β\_0, β\_1를 구하기



### 산 정상 오르기

**문제**: 아무 곳에서나 시작했을 때, 가장 정상을 빠르게 찾아가는 방법은?

![image-20220323135741976](assets/image-20220323135741976.png)

##### 가정

- 정상의 위치는 알 수 없다.
- 현재 나의 위치와 높이를 알 수 있다.
- 내 위치에서 일정 수준 이동할 수 있다.

##### 방법

- 현재 위치에서 가장 경사가 높은 쪽을 찾는다
- 오르막 방향으로 일정 수준 이동한다
- 더 이상 높이의 변화가 없을 때까지 반복



### Gradient Descent(경사하강법)

**거꾸로 된 산을 내려가기**

- 최적의 값을 찾기 위한 거꾸로 된 산을 내려가는 방법
- <u>Loss function</u>을 최소로 만드는 β\_0, β\_1을 선정함



### 회귀 분석 개념 정리하기

**Loss Function(실제 값과 모델이 예측하는 값의 오차)**을 최소화하는 **Gradient Descent(최적의 β\_0, β\_1를 찾는 알고리즘)**를 통해 데이터를 가장 잘 설명할 수 있는 선을 찾는 방법



## 2. 단순 선형회귀

> Simple Linear Regression

### 단순 선형 회귀 (Simple Linear Regression)

가장 기본적이고 간단한 방법의 회귀 알고리즘

입력값 X와 결과값 Y의 관계를 설명할 때 가장 많이사용되는 단순한 모델

**단순 선형회귀의 함수식**
$$
Y\approx\beta_0 + \beta_1X
$$
**단순 선형회귀의 Loss Function**
$$
\sum (y^{(i)}-(\beta_0 + \beta_1x^{(i)}))^2
$$




### 단순 선형회귀 특징

- 가장 기초적이나 여전히 많이 사용되는 알고리즘
- 입력값(X)이 1개인 경우에만 적용이 가능함
- 입력값과 결과값의 관계를 알아보는데 용이함
- 입력값이 결과값에 얼마나 영향을 미치는지 알 수 있음
- 두 변수 간의 관계를 직관적으로 해석하고자 하는 경우 활용



## 3. 다중 선형회귀와 다항 회귀

> Multiple Linear Regression, Polynomial Regression

### 다중 선형 회귀 (Multiple Linear Regression)

입력값 X가 여러개(2개 이상)인 경우 활용할 수 있는 회귀 알고리즘

각 개별 X_i에 해당하는 최적의 β\_i를 찾아야 함

**다중 선형회귀의 함수식**
$$
Y\approx\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_iX_i
$$
**다중 선형회귀의 Loss Function**
$$
\sum (y^{(i)}-(\beta_0 + \beta_1x_1^{(i)} + \beta_2x_2^{(i)} + ... + \beta_nx_n^{(i)}))^2
$$


### 다중 선형 회귀 특징

- 여러 개의 입력값과 결과값 간의 관계 확인 가능

- 어떤 입력값이 결과값에 어떠한 영향을 미치는지 알 수 있음
- 여러 개의 입력값 사이의 **상관 관계**가 높을 경우 결과에 대한 신뢰성을 잃을 가능성이 있음

✅ 상관관계: 두 가지 것의 한쪽이 변화하면 다른 한쪽도 따라서 변화하는 관계



### 다항 회귀 (Polynomial Regression)

1차 함수 선형식으로 표현하기 어려운 분포의 데이터를 위한 회귀

복잡한 분포의 데이터의 경우 일반 선형 회귀 알고리즘 적용 시 낮은 성능의 결과가 도출됨

**다항 회귀의 함수식**
$$
Y\approx\beta_0 + \beta_1X_1 + \beta_2X_2^2 + ... + \beta_iX_i^i
$$


### 다항 회귀 특징

- 일차 함수 식으로 표현할 수 없는 복잡한 데이터 분포에도 적용 가능
- 극단적으로 높은 차수의 모델을 구현할 경우 과도하게 학습 데이터에 맞춰지는 과적합 현상 발생
- 데이터 관계를 선형으로 표현하기 어려운 경우 사용



## 4. 과적합과 정규화 

> Overfitting, Cross Validation, Regularization

### 과적합 (Overfitting)

모델이 주어진 훈련 데이터에 과도하게 맞춰져 새로운 데이터가 입력 되었을 때 잘 예측하지 못하는 현상

즉, 모델이 과도하게 복잡해져 일반성이 떨어진 경우를 의미함

![image-20220323142753038](assets/image-20220323142753038.png)



### 과적합 방지 방법

모델이 잘 적합되어 실제 데이터와 유사한 예측 결과를 얻을 수 있도록 과적합 방지를 위해 다양한 방법을 사용함

**예시**: 교차 검증(Cross Validation), 정규화(Regularization)



### 교차 검증 (Cross Validation)

모델이 잘 적합되었는지 알아보기 위해 **훈련용 데이터**와 별개의 **테스트 데이터**, 그리고 **검증 데이터**로 나누어 성능 평가하는 방법

일반적으로 **K-fold 교차 검증**을 많이 사용함